{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "999fa95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lisa/anaconda3/envs/llm_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, set_seed\n",
    "from peft import LoraConfig, PeftModel\n",
    "from datasets import load_dataset\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datetime import datetime\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cc94a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_3_1 = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "PROJECT_NAME = \"llama_finetune\"\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "QUANT_4_BIT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4216c105",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "if not hf_token:\n",
    "    raise ValueError(\"❌ HF_TOKEN not found in environment variables.\")\n",
    "login(token=hf_token, add_to_git_credential=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6dc0aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ed-donner/new-pricer-data\")\n",
    "train = dataset['train']\n",
    "test = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "031cc90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_NAME = f\"{datetime.now():%y-%m-%d_%H.%M.%S}\"\n",
    "PROJECT_RUN_NAME = f\"{PROJECT_NAME}-{RUN_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69fa3c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA hyperparameters\n",
    "LORA_R = 32          # rank dimension\n",
    "LORA_ALPHA = 64      # scaling factor\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "LORA_DROPOUT = 0.1\n",
    "QUANT_4_BIT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1672663",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "BATCH_SIZE = 16\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "LR_SCHEDULER_TYPE = 'cosine'\n",
    "WARMUP_RATIO = 0.03\n",
    "OPTIMIZER = \"paged_adamw_32bit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d086d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS = 50\n",
    "SAVE_STEPS = 5000\n",
    "LOG_TO_WANDB = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d2fef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "if QUANT_4_BIT:\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,               \n",
    "        bnb_4bit_use_double_quant=True, \n",
    "        bnb_4bit_quant_type=\"nf4\",      \n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "else:\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        bnb_8bit_compute_dtype=torch.float16\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be57bdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded with 5.6 GB allocated\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA_3_1, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA_3_1,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "print(f\"✅ Model loaded with {base_model.get_memory_footprint()/1e9:,.1f} GB allocated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16c40c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_parameters = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",  # language modeling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a28592a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use bf16 only if supported\n",
    "use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "955e1b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_parameters = SFTConfig(\n",
    "    output_dir=PROJECT_RUN_NAME,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=1,\n",
    "    eval_strategy=\"no\",                     # <-- fixed\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    optim=OPTIMIZER,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=10,\n",
    "    logging_steps=STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "    report_to=\"wandb\" if LOG_TO_WANDB else None,\n",
    "    run_name=RUN_NAME,\n",
    "    save_strategy=\"steps\",\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1eea37b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    args=train_parameters,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    peft_config=lora_parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e712179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': None}.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb047b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
